{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4f8c2ae9-9072-4060-92d4-03c5f7bae306",
   "metadata": {},
   "source": [
    "Basically here were are importing the transformers which will be required i.e. BertTokenizer\n",
    "also install this:\n",
    "pip install transformers torch datasets\n",
    "although datasets is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5c7aad-e4c2-4938-b7ba-d3ac363cc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4381388-76f1-4529-9968-4a2e4295eec8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/ASHISH MEENA/OneDrive/Desktop/Project/Face/archive/pii_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m dt \u001b[38;5;241m=\u001b[39m rd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/ASHISH MEENA/OneDrive/Desktop/Project/Face/archive/pii_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(dt\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      5\u001b[0m dt_train, dt_test \u001b[38;5;241m=\u001b[39m train_test_split(dt, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.125\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/ASHISH MEENA/OneDrive/Desktop/Project/Face/archive/pii_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as rd\n",
    "from sklearn.model_selection import train_test_split\n",
    "dt = rd.read_csv('C:/Users/ASHISH MEENA/OneDrive/Desktop/Project/Face/archive/pii_dataset.csv')\n",
    "print(dt.head())\n",
    "dt_train, dt_test = train_test_split(dt, test_size=0.125, random_state = 7)\n",
    "print(f\"Training data size: {len(dt_train)}\")\n",
    "print(f\"Test data size: {len(dt_test)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fafdf789-6582-431d-a551-6f97af3f19df",
   "metadata": {},
   "source": [
    "Now we want to find our BIO labels lenght, and we don't know about it, so we will calculate it by going through the whole column of the BIO-labels given in the complete dataset (There is no data leakage in this case although we are using the whole dataset, since we are just making calculation about the total number of labels that our valid for our model)\n",
    "So let's calculate the number of the bio labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7748ccb-e1c9-4dfa-aa4d-fd181d9a2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...\n",
      "1    ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...\n",
      "2    ['O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O',...\n",
      "3    ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...\n",
      "4    ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...\n",
      "Name: labels, dtype: object\n",
      "{'B-STREET_ADDRESS': 0, 'O': 1, 'B-NAME_STUDENT': 2, 'I-NAME_STUDENT': 3, 'B-PHONE_NUM': 4, 'B-EMAIL': 5, 'B-USERNAME': 6, 'I-STREET_ADDRESS': 7, 'I-PHONE_NUM': 8, 'B-URL_PERSONAL': 9}\n",
      "Number of labels : 10\n"
     ]
    }
   ],
   "source": [
    "BIO_labels = dt['labels'].tolist()\n",
    "#saare unique ko extract\n",
    "print(dt['labels'].head())\n",
    "# print(BIO_labels[:2]) \n",
    "\n",
    "BIO_labels_temp = BIO_labels\n",
    "#i found out that the BIO_labels weren't as lists, they were in the form of strings, so used the following code (from gpt) to make it convinient for our use\n",
    "import ast\n",
    "BIO_labels = [ast.literal_eval(label) if isinstance(label, str) else label for label in dt['labels']]\n",
    "\n",
    "# print(BIO_labels[:2])  # Should show lists like ['B-NAME', 'I-NAME', 'O']\n",
    "\n",
    "# now proceeding by finding the unique labels\n",
    "unique_labels = set(label for sublist in BIO_labels for label in sublist)\n",
    "#integer value specify\n",
    "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "print(label_map)\n",
    "print(f\"Number of labels : {len(label_map)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2df5b50b-8729-42fa-a13c-a85b445a8946",
   "metadata": {},
   "source": [
    "Now we have to proceed to preparation of data for training, since BERT needs the input in the format as:\n",
    "Converted tokens and BIO labels into a format suitable\n",
    "Attention masks is also required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ae04b64-4def-4b8b-bc09-670800bfd8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "0       363\n",
      "1       255\n",
      "2       259\n",
      "3       281\n",
      "4       210\n",
      "       ... \n",
      "4429    360\n",
      "4430    248\n",
      "4431    445\n",
      "4432    320\n",
      "4433    176\n",
      "Length: 4434, dtype: int64\n",
      "max length :\n",
      "538\n"
     ]
    }
   ],
   "source": [
    "#first i'll be checking if the BIO_labels have same sizes or not\n",
    "BIO_labels_temp = rd.Series(BIO_labels)\n",
    "lengths = BIO_labels_temp.apply(len)\n",
    "maxi = max(lengths)\n",
    "if lengths.nunique() == 1:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")\n",
    "    print(lengths)\n",
    "\n",
    "print(f\"max length :\")\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1cbae8a-a7bf-49ae-9edc-ad54c8f28bef",
   "metadata": {},
   "source": [
    "Hence we have to do the padding for our dataset to make it useful for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4297f6b9-bf20-4869-9995-36cee6381693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS']\", \"['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'O', 'O', 'O', 'O']\"]\n",
      "1413\n",
      "['O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'B-EMAIL']\n"
     ]
    }
   ],
   "source": [
    "tok = dt['labels'].tolist()\n",
    "print(tok[:2])\n",
    "l = len(tok[2])\n",
    "print(l)\n",
    "print(tok[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a4b51ae-30e8-417a-93c2-2b1c60fa9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(tokens, maxi):\n",
    "    return tokens + ['[PAD]'] * (maxi - len(tokens))\n",
    "def pad_bio(BIO, maxi):\n",
    "    return BIO + ['O'] * (maxi - len(BIO))\n",
    "\n",
    "padded_tokens = []\n",
    "padded_BIO = []\n",
    "\n",
    "tik = dt_train['tokens'].tolist()\n",
    "tik = [ast.literal_eval(label) if isinstance(label, str) else label for label in dt['tokens']]\n",
    "for token_list in tik:\n",
    "    padd = pad_tokens(token_list, maxi)\n",
    "    padded_tokens.append(padd)\n",
    "\n",
    "for token_list in BIO_labels:\n",
    "    padd = pad_bio(token_list, maxi)\n",
    "    padded_BIO.append(padd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9926b3a1-cf3d-4c2a-8902-b44cf1af8e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PHONE_NUM', 'I-PHONE_NUM', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# print(padded_tokens[:1])\n",
    "print(padded_BIO[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c510e25-2159-48f2-8e2a-d0cc21762c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "538\n",
      "yes\n",
      "538\n"
     ]
    }
   ],
   "source": [
    "#to check if the padding is done properly or not\n",
    "\n",
    "BIO_labels_temp = rd.Series(padded_tokens)\n",
    "lengths = BIO_labels_temp.apply(len)\n",
    "if lengths.nunique() == 1:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")\n",
    "    print(lengths)\n",
    "\n",
    "maxi = max(lengths)\n",
    "print(maxi)\n",
    "\n",
    "BIO_labels_temp = rd.Series(padded_BIO)\n",
    "lengths = BIO_labels_temp.apply(len)\n",
    "if lengths.nunique() == 1:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")\n",
    "    print(lengths)\n",
    "\n",
    "maxi = max(lengths)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f88fa991-9816-401f-b737-06e35b019ef3",
   "metadata": {},
   "source": [
    "Now we will be generating the token ids (this is the last step for pre data process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c162942f-ad82-4c50-b2d0-87a216b86ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([1024, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertConfig\n",
    "\n",
    "# Load the base BERT configuration\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Increase the maximum position embeddings to a higher number (e.g., 1024)\n",
    "config.max_position_embeddings = 1024\n",
    "\n",
    "# Load the model with the new config, allowing mismatched sizes\n",
    "# model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Create a tokenizer that matches the new config\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", model_max_length=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "59c45df8-361e-4396-aa8b-5652d030a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100, 2171, 2003, 100, 100, 1998, 100, 2572, 1037, 100, 2007, 2410, 2086, 1997, 100, 100, 3342, 1037, 2200, 4310, 1998, 10368, 2622, 100, 2018, 2000, 2147, 2006, 2197, 100, 100, 8013, 5411, 2033, 2007, 1037, 9062, 2155, 100, 1011, 1037, 6323, 13016, 2008, 2018, 2042, 2979, 2091, 2083, 100, 100, 1996, 13016, 2001, 1999, 3532, 100, 2007, 2195, 6065, 11719, 1998, 1037, 3714, 100, 100, 8013, 2359, 2033, 2000, 9239, 2009, 2000, 2049, 2280, 100, 2021, 2009, 2001, 3154, 2008, 2023, 2052, 2022, 2053, 6623, 100, 100, 2026, 7772, 5906, 1998, 100, 100, 2211, 1996, 10059, 4708, 1997, 100, 1996, 100, 100, 6323, 2001, 5362, 3718, 2013, 2049, 100, 1998, 1996, 5591, 23465, 2001, 100, 100, 1996, 13016, 2001, 3294, 100, 100, 100, 12176, 2169, 6323, 1998, 20456, 2009, 2005, 2151, 100, 100, 1996, 11719, 2020, 2035, 1999, 2204, 100, 2007, 2053, 15288, 2030, 100, 100, 2279, 3357, 2001, 2000, 7192, 1996, 3714, 100, 100, 5362, 100, 1996, 3714, 4109, 2067, 100, 12725, 2008, 1996, 23465, 2001, 23073, 1998, 100, 100, 1996, 23465, 2001, 100, 100, 2211, 1996, 2832, 1997, 100, 1996, 100, 100, 6323, 2001, 5362, 2872, 2067, 2046, 2049, 100, 1998, 1996, 13016, 2001, 12853, 2127, 2009, 28092, 2066, 100, 100, 100, 3591, 1996, 5854, 13016, 2000, 1996, 100, 2027, 2020, 100, 100, 100, 2903, 2008, 100, 2018, 2042, 2583, 2000, 3288, 2037, 2155, 100, 2067, 2000, 100, 100, 13016, 2246, 2004, 3376, 2004, 2009, 2018, 2043, 2009, 2001, 2034, 100, 1998, 1996, 8013, 2001, 16082, 2000, 2031, 2009, 2067, 1999, 2037, 100, 100, 2017, 2031, 1037, 2622, 2008, 2017, 2052, 2066, 2000, 100, 3531, 2514, 2489, 2000, 3967, 2033, 2011, 3042, 2012, 100, 100, 2030, 2011, 10373, 2012, 100, 100, 2298, 2830, 2000, 4994, 2013, 100, 100, 100, 100, 2025, 4526, 3376, 100, 100, 5959, 5938, 2051, 100, 100, 2293, 6631, 2026, 3716, 2055, 11912, 1998, 7176, 2007, 2060, 2111, 2040, 2024, 13459, 2055, 2023, 2396, 100, 100, 2036, 5959, 5938, 2051, 2007, 2026, 2155, 1998, 11131, 2047, 100, 100, 2017, 2052, 2066, 2000, 4553, 2062, 2055, 100, 3531, 2514, 2489, 2000, 3942, 2026, 4037, 2012, 100, 100, 2030, 3942, 2033, 2012, 2026, 2996, 2284, 2012, 5989, 100, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[['My', 'name', 'is', 'Aaliyah', 'Popova,', 'and', 'I', 'am', 'a', 'jeweler', 'with', '13', 'years', 'of', 'experience.', 'I', 'remember', 'a', 'very', 'unique', 'and', 'challenging', 'project', 'I', 'had', 'to', 'work', 'on', 'last', 'year.', 'A', 'customer', 'approached', 'me', 'with', 'a', 'precious', 'family', 'heirloom', '-', 'a', 'diamond', 'necklace', 'that', 'had', 'been', 'passed', 'down', 'through', 'generations.', 'Unfortunately,', 'the', 'necklace', 'was', 'in', 'poor', 'condition,', 'with', 'several', 'loose', 'diamonds', 'and', 'a', 'broken', 'clasp.', 'The', 'customer', 'wanted', 'me', 'to', 'restore', 'it', 'to', 'its', 'former', 'glory,', 'but', 'it', 'was', 'clear', 'that', 'this', 'would', 'be', 'no', 'ordinary', 'repair.', 'Using', 'my', 'specialized', 'tools', 'and', 'techniques,', 'I', 'began', 'the', 'delicate', 'task', 'of', 'dismantling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'removed', 'from', 'its', 'setting,', 'and', 'the', 'damaged', 'clasp', 'was', 'removed.', 'Once', 'the', 'necklace', 'was', 'completely', 'disassembled,', 'I', 'meticulously', 'cleaned', 'each', 'diamond', 'and', 'inspected', 'it', 'for', 'any', 'damage.', 'Fortunately,', 'the', 'diamonds', 'were', 'all', 'in', 'good', 'condition,', 'with', 'no', 'cracks', 'or', 'chips.', 'The', 'next', 'step', 'was', 'to', 'repair', 'the', 'broken', 'clasp.', 'I', 'carefully', 'soldered', 'the', 'broken', 'pieces', 'back', 'together,', 'ensuring', 'that', 'the', 'clasp', 'was', 'sturdy', 'and', 'secure.', 'Once', 'the', 'clasp', 'was', 'repaired,', 'I', 'began', 'the', 'process', 'of', 'reassembling', 'the', 'necklace.', 'Each', 'diamond', 'was', 'carefully', 'placed', 'back', 'into', 'its', 'setting,', 'and', 'the', 'necklace', 'was', 'polished', 'until', 'it', 'sparkled', 'like', 'new.', 'When', 'I', 'presented', 'the', 'restored', 'necklace', 'to', 'the', 'customer,', 'they', 'were', 'overjoyed.', 'They', \"couldn't\", 'believe', 'that', 'I', 'had', 'been', 'able', 'to', 'bring', 'their', 'family', 'heirloom', 'back', 'to', 'life.', 'The', 'necklace', 'looked', 'as', 'beautiful', 'as', 'it', 'had', 'when', 'it', 'was', 'first', 'created,', 'and', 'the', 'customer', 'was', 'thrilled', 'to', 'have', 'it', 'back', 'in', 'their', 'possession.', 'If', 'you', 'have', 'a', 'project', 'that', 'you', 'would', 'like', 'to', 'discuss,', 'please', 'feel', 'free', 'to', 'contact', 'me', 'by', 'phone', 'at', '(95)', '94215-7906', 'or', 'by', 'email', 'at', 'aaliyah.popova4783@aol.edu.', 'I', 'look', 'forward', 'to', 'hearing', 'from', 'you!', 'P.S.:', 'When', \"I'm\", 'not', 'creating', 'beautiful', 'jewelry,', 'I', 'enjoy', 'spending', 'time', 'podcasting.', 'I', 'love', 'sharing', 'my', 'knowledge', 'about', 'jewelry', 'and', 'connecting', 'with', 'other', 'people', 'who', 'are', 'passionate', 'about', 'this', 'art', 'form.', 'I', 'also', 'enjoy', 'spending', 'time', 'with', 'my', 'family', 'and', 'exploring', 'new', 'places.', 'If', 'you', 'would', 'like', 'to', 'learn', 'more', 'about', 'me,', 'please', 'feel', 'free', 'to', 'visit', 'my', 'website', 'at', '[website', 'address]', 'or', 'visit', 'me', 'at', 'my', 'studio', 'located', 'at', '97', 'Lincoln', 'Street.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "token_ids = []\n",
    "for sublist in padded_tokens: \n",
    "    token_ids.append(tokenizer.convert_tokens_to_ids(sublist))\n",
    "print(token_ids[:1])\n",
    "print(padded_tokens[:1])\n",
    "def create_attention_mask(padded_token_ids, padding_token_id=0):\n",
    "    attention_masks = []\n",
    "    for seq in padded_token_ids:\n",
    "        # Create attention mask: 1 for real tokens, 0 for padding tokens\n",
    "        mask = [1 if token != padding_token_id else 0 for token in seq]\n",
    "        attention_masks.append(mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "039cd9cc-a247-4583-93f7-98340a61ea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "attention_masks = create_attention_mask(token_ids)\n",
    "print(attention_masks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "237c592f-9046-4c2b-87d6-9bd9f76a4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens([2410])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4aeb321e-ffa6-4972-be8f-84911e90c57c",
   "metadata": {},
   "source": [
    "Now everything is ready, now we need dataloader to help us to feed data into our model, we will use torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f3436228-4e1e-4c23-87b0-b240723c2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor(token_ids)          # Shape: (batch_size, sequence_length)\n",
    "attention_mask = torch.tensor(attention_masks)  # Shape: (batch_size, sequence_length)\n",
    "# the following line gave error since i forgot to use the bio labels which we evaluated earlier in place of strings\n",
    "# labels = torch.tensor(padded_BIO)      # Shape: (batch_size, sequence_length)\n",
    "\n",
    "BIO_LABEL_MAP = label_map\n",
    "\n",
    "numerical_BIO_labels = [\n",
    "    [BIO_LABEL_MAP.get(label, -1) for label in entry]  # Use -1 for unknown labels (if any)\n",
    "    for entry in padded_BIO\n",
    "]\n",
    "\n",
    "labels = torch.tensor(numerical_BIO_labels)\n",
    "\n",
    "# Now we can create the final input batch\n",
    "batch = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    \"labels\": labels\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db586fb8-02a9-47bd-80e7-3de7518550a0",
   "metadata": {},
   "source": [
    "Now we have to do create our dataset which will be used for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "14770ed7-7540-49ac-806b-c8d19d87eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PiiDataset(Dataset):\n",
    "    def __init__(self, token_ids, attention_masks, labels):\n",
    "        self.token_ids = token_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.token_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# now the dataloader:\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the dataset instance\n",
    "dataset = PiiDataset(token_ids, attention_masks, numerical_BIO_labels)\n",
    "\n",
    "# Create DataLoader (you can adjust batch_size as needed)\n",
    "batch_size = 16  # You can change this according to your memory limits\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b328050a-7b91-4a70-b649-a05dc18ae7d6",
   "metadata": {},
   "source": [
    "Now its time to train our model (BERT model fine tuning), so to do this we will simply use our dataloader and import some required libraries as:\n",
    "also first lets load our bert model which is pre trained, we will fine tune it for our purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "664dfa5a-24c0-45c6-bcd4-a3a6a6ffc5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "unique_labels = label_map # or whatever the appropriate values are\n",
    "num_labels = len(unique_labels)  # Number of unique labels\n",
    "\n",
    "# Load the model configuration\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Update the number of labels in the model config\n",
    "config.num_labels = num_labels\n",
    "\n",
    "# Load the model with the updated configuration\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dbe0966-baee-4360-9cf3-140042b5a147",
   "metadata": {},
   "source": [
    "now lets start the training process using the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "373f2004-f5b1-4420-b194-4cabc6db904b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 538])\n",
      "1024\n",
      "10\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(input_ids.shape)\n",
    "print(config.max_position_embeddings)  # Should print 1024\n",
    "print(config.num_labels) \n",
    "print(model.config.max_position_embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2c9b0-1e30-4984-9e4d-3c4daa090cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/3:   9%|â–Š         | 24/278 [14:43<2:37:18, 37.16s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you already have the model loaded, optimizer, and other components\n",
    "# Example:\n",
    "# model = ...  # Your pre-trained model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  # You can adjust learning rate\n",
    "# You can also use a learning rate scheduler if necessary\n",
    "# scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0  # To accumulate the loss for the epoch\n",
    "\n",
    "    # Loop over batches in DataLoader\n",
    "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "        # Move batch data to device (GPU/CPU)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Zero the gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # Get the loss (it's usually the first element in the model output)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step (update model weights)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Optional: Scheduler step\n",
    "        # scheduler.step()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "439ba14f-3e8c-4be0-ac5f-db73faf6732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 538])\n",
      "1024\n",
      "10\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(config.max_position_embeddings)  # Should print 1024\n",
    "print(config.num_labels) \n",
    "print(model.config.max_position_embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a9f15-6669-4aa0-9bfb-5e3a3c977066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
